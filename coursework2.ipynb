{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1 - Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: 25/03/2025\n",
    "\n",
    "## CID: insert your CID here\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Quick questions [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:\n",
    "\n",
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "***\n",
    "\n",
    "## Exercise 2: [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 1 [2 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2 [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 3 [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Exercise 3: Implementation [13 points]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:43.455625Z",
     "start_time": "2025-03-17T23:13:41.048140Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Image classification [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 1 [0.5 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Download the dataset from\n",
    "https://chaladze.com/l5/img/Linnaeus%205%2032X32.rar\n",
    "and uncompress the .rar dataset to a folder named \"Linnaeus_5_32X32\" in the current directory.\n",
    "This folder contains a subfolder \"Linnaeus 5 32X32\", which contains a test and a train folder with the images of the dataset in 5 different classes: berry, bird, dog, flower, other."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:43.477082Z",
     "start_time": "2025-03-17T23:13:43.457369Z"
    }
   },
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=\"Linnaeus_5_32X32\\\\Linnaeus 5 32X32\\\\train\", transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=\"Linnaeus_5_32X32\\\\Linnaeus 5 32X32\\\\test\", transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False,)\n",
    "\n",
    "classes = ('berry', 'bird', 'dog', 'flower', 'other')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:43.577651Z",
     "start_time": "2025-03-17T23:13:43.572054Z"
    }
   },
   "cell_type": "code",
   "source": "trainset.classes",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['berry', 'bird', 'dog', 'flower', 'other']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:44.970448Z",
     "start_time": "2025-03-17T23:13:43.587159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "to_display = {cls: None for cls in trainset.classes}\n",
    "idx_to_class = {i: c for c, i in trainset.class_to_idx.items()}\n",
    "i = 0\n",
    "j = 0\n",
    "for img, label in trainset:\n",
    "    if to_display[idx_to_class[label]] is None:\n",
    "        to_display[idx_to_class[label]] = img\n",
    "        i += 1\n",
    "    if i >= len(trainset.classes):\n",
    "        break\n",
    "\n",
    "fig, axs = plt.subplots(1, len(trainset.classes), figsize=(15, 3))\n",
    "for i in range(len(trainset.classes)):\n",
    "    cls = trainset.classes[i]\n",
    "    axs[i].imshow(to_display[cls].numpy().transpose(1, 2, 0))\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(cls)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.9764706].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.827451..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.99215686..0.7647059].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..0.827451].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.90588236..0.92156863].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Question 2 [0.5 point]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor (),W\n",
    "    transforms . Normalize ((0.5 , 0.5 , 0.5) , (0.5 , 0.5 , 0.5)\n",
    ")]\n",
    "```\n",
    "The above transform first takes an input image (either a PIL image or numpy array of shape (H, W, C)) and converts it into a torch tensor of shape (C, H, W) whilst scaling from [0, 255] to [0, 1]. Then the normalise transform applies channel-wise normalisation for each pixel. It computes $x_{norm} = \\frac{x-0.5}{0.5}$. This transforms the pixel ranges from [0, 1] to [-1, 1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 3 [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:44.977029Z",
     "start_time": "2025-03-17T23:13:44.970448Z"
    }
   },
   "source": [
    "class myCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.mp3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=4096, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # cnn\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.mp1(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.mp2(x)\n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.relu(self.conv6(x))\n",
    "        x = self.mp3(x)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(-1, 4096)\n",
    "\n",
    "        # fcn\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 4 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.523839Z",
     "start_time": "2025-03-17T23:13:44.986656Z"
    }
   },
   "source": [
    "import torchsummary\n",
    "\n",
    "torchsummary.summary(myCNN(), (3, 32, 32))"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchsummary\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mtorchsummary\u001B[49m\u001B[43m.\u001B[49m\u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmyCNN\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m32\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torchsummary\\torchsummary.py:72\u001B[39m, in \u001B[36msummary\u001B[39m\u001B[34m(model, input_size, batch_size, device)\u001B[39m\n\u001B[32m     68\u001B[39m model.apply(register_hook)\n\u001B[32m     70\u001B[39m \u001B[38;5;66;03m# make a forward pass\u001B[39;00m\n\u001B[32m     71\u001B[39m \u001B[38;5;66;03m# print(x.shape)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[38;5;66;03m# remove these hooks\u001B[39;00m\n\u001B[32m     75\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m hooks:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 22\u001B[39m, in \u001B[36mmyCNN.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m# cnn\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.relu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     23\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.relu(\u001B[38;5;28mself\u001B[39m.conv2(x))\n\u001B[32m     24\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.mp1(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1842\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1844\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1845\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1846\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1847\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1848\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1849\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n\u001B[32m   1850\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m _global_forward_hooks.items():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1790\u001B[39m     bw_hook = BackwardHook(\u001B[38;5;28mself\u001B[39m, full_backward_hooks, backward_pre_hooks)\n\u001B[32m   1791\u001B[39m     args = bw_hook.setup_input_hook(args)\n\u001B[32m-> \u001B[39m\u001B[32m1793\u001B[39m result = \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1794\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks:\n\u001B[32m   1795\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook_id, hook \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[32m   1796\u001B[39m         *_global_forward_hooks.items(),\n\u001B[32m   1797\u001B[39m         *\u001B[38;5;28mself\u001B[39m._forward_hooks.items(),\n\u001B[32m   1798\u001B[39m     ):\n\u001B[32m   1799\u001B[39m         \u001B[38;5;66;03m# mark that always called hook is run\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 5 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.523839600Z",
     "start_time": "2025-03-17T21:03:56.460759Z"
    }
   },
   "source": [
    "def train(myCNN, nr_epochs, optimizer ,criterion, trainloader):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "\n",
    "    for epoch in range(nr_epochs):\n",
    "        for inputs, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = myCNN(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch, nr_epochs, loss.item()))\n",
    "\n",
    "    model.to('cpu')\n",
    "    criterion.to('cpu')"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 6 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.523839600Z",
     "start_time": "2025-03-17T21:03:57.267438Z"
    }
   },
   "source": [
    "model = myCNN()\n",
    "\n",
    "n_epochs = 30\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, n_epochs, optim, criterion, trainloader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/30], Loss: 1.4583\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m optim = torch.optim.Adam(model.parameters(), lr=\u001B[32m0.001\u001B[39m)\n\u001B[32m      5\u001B[39m criterion = nn.CrossEntropyLoss()\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(myCNN, nr_epochs, optimizer, criterion, trainloader)\u001B[39m\n\u001B[32m      5\u001B[39m criterion.to(device)\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(nr_epochs):\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    706\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    707\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m708\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    709\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    711\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    712\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    713\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    714\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    762\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    763\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m764\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    765\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    766\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001B[39m, in \u001B[36mDatasetFolder.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m    245\u001B[39m sample = \u001B[38;5;28mself\u001B[39m.loader(path)\n\u001B[32m    246\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m247\u001B[39m     sample = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.target_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    249\u001B[39m     target = \u001B[38;5;28mself\u001B[39m.target_transform(target)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001B[39m, in \u001B[36mToTensor.__call__\u001B[39m\u001B[34m(self, pic)\u001B[39m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[32m    130\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    131\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    135\u001B[39m \u001B[33;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[32m    136\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\mfml\\env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001B[39m, in \u001B[36mto_tensor\u001B[39m\u001B[34m(pic)\u001B[39m\n\u001B[32m    170\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pic.mode == \u001B[33m\"\u001B[39m\u001B[33m1\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    171\u001B[39m     img = \u001B[32m255\u001B[39m * img\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m img = \u001B[43mimg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpic\u001B[49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mF_pil\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_image_num_channels\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[38;5;66;03m# put it from HWC to CHW format\u001B[39;00m\n\u001B[32m    174\u001B[39m img = img.permute((\u001B[32m2\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m)).contiguous()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 7 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.528780400Z",
     "start_time": "2025-03-17T20:51:47.369353Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 2: Fine-tuning a pretrained model [3.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.528780400Z",
     "start_time": "2025-03-17T20:51:47.380224Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 2 [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.532792600Z",
     "start_time": "2025-03-17T20:51:47.390807Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 3 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.534913100Z",
     "start_time": "2025-03-17T20:51:47.401484Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 4 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.535415600Z",
     "start_time": "2025-03-17T20:51:47.410713Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 5 [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.535415600Z",
     "start_time": "2025-03-17T20:51:47.420639Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Part 3: Adversarial attacks [4.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 1 [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.535415600Z",
     "start_time": "2025-03-17T20:51:47.432694Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 2 [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 3 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.535415600Z",
     "start_time": "2025-03-17T20:51:47.444919Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 4 [0.5 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.535415600Z",
     "start_time": "2025-03-17T20:51:47.454197Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 5 [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:13:45.535415600Z",
     "start_time": "2025-03-17T20:51:47.464093Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Question 6 [0.5 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
